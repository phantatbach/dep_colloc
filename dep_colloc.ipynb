{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dep Path Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Regex to extract tab-separated token info\n",
    "pattern = re.compile(r'([^\\t]+)\\t([^\\t]+)\\t([^\\t])[^\\t]*\\t([^\\t]+)\\t([^\\t]+)\\t([^\\t]+)')\n",
    "\n",
    "def build_dependency_tree(tokens):\n",
    "    tree = defaultdict(list)\n",
    "    roots = []\n",
    "    for tok in tokens:\n",
    "        m = pattern.match(tok)\n",
    "        if m:\n",
    "            _, _, _, idx, head, _ = m.groups()\n",
    "            if head == \"0\":\n",
    "                roots.append(idx)\n",
    "            else:\n",
    "                tree[head].append(idx)\n",
    "    return tree, roots\n",
    "\n",
    "def get_max_depth(tree, root):\n",
    "    visited = set()\n",
    "    stack = [(root, 1)]\n",
    "    max_depth = 1\n",
    "\n",
    "    while stack:\n",
    "        node, depth = stack.pop()\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        max_depth = max(max_depth, depth)\n",
    "        for child in tree.get(node, []):\n",
    "            stack.append((child, depth + 1))\n",
    "\n",
    "    return max_depth\n",
    "\n",
    "def analyze_max_depth_distribution(corpus_dir):\n",
    "    depths = []\n",
    "\n",
    "    files = [f for f in os.listdir(corpus_dir) if f.endswith(\".txt\")]\n",
    "    for fname in tqdm(files, desc=\"Analyzing syntactic depths\"):\n",
    "        with open(os.path.join(corpus_dir, fname), encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        sentence = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"<s\"):\n",
    "                sentence = []\n",
    "            elif line.startswith(\"</s>\"):\n",
    "                tree, roots = build_dependency_tree(sentence)\n",
    "                for root in roots:\n",
    "                    depth = get_max_depth(tree, root)\n",
    "                    depths.append(depth)\n",
    "            elif line:\n",
    "                sentence.append(line)\n",
    "    return depths\n",
    "\n",
    "def plot_depth_distribution(depths, save_path=None, min_count=1):\n",
    "    # Count frequencies\n",
    "    depth_counter = Counter(depths)\n",
    "\n",
    "    # Apply threshold filter\n",
    "    filtered = [d for d in depths if depth_counter[d] >= min_count]\n",
    "    if not filtered:\n",
    "        print(f\"[!] No depths with frequency ≥ {min_count}\")\n",
    "        return\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(filtered, bins=range(1, max(filtered)+2), edgecolor='black')\n",
    "    plt.title(f\"Distribution of Max Syntactic Depth (min_count ≥ {min_count})\")\n",
    "    plt.xlabel(\"Max Depth\")\n",
    "    plt.ylabel(\"Number of Sentences\")\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"[✓] Plot saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nSummary Stats:\")\n",
    "    print(f\"  Min: {min(filtered)}\")\n",
    "    print(f\"  Max: {max(filtered)}\")\n",
    "    print(f\"  Mean: {np.mean(filtered):.2f}\")\n",
    "    print(f\"  Median: {np.median(filtered):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = analyze_max_depth_distribution(\"/home/volt/bach/pilot_data/COHA/10_20_parsed_1_SPOS\")\n",
    "plot_depth_distribution(depths, save_path=\"./depth_distribution.png\", min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_files_with_depth_threshold(corpus_dir, target_depth):\n",
    "    import os\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    pattern = re.compile(r'([^\\t]+)\\t([^\\t]+)\\t([^\\t])[^\\t]*\\t([^\\t]+)\\t([^\\t]+)\\t([^\\t]+)')\n",
    "\n",
    "    def build_dependency_tree(tokens):\n",
    "        tree = defaultdict(list)\n",
    "        roots = []\n",
    "        for tok in tokens:\n",
    "            m = pattern.match(tok)\n",
    "            if m:\n",
    "                _, _, _, idx, head, _ = m.groups()\n",
    "                if head == \"0\":\n",
    "                    roots.append(idx)\n",
    "                else:\n",
    "                    tree[head].append(idx)\n",
    "        return tree, roots\n",
    "\n",
    "    def get_max_depth(tree, root):\n",
    "        visited = set()\n",
    "        stack = [(root, 1)]\n",
    "        max_depth = 1\n",
    "        while stack:\n",
    "            node, depth = stack.pop()\n",
    "            if node in visited:\n",
    "                continue\n",
    "            visited.add(node)\n",
    "            max_depth = max(max_depth, depth)\n",
    "            for child in tree.get(node, []):\n",
    "                stack.append((child, depth + 1))\n",
    "        return max_depth\n",
    "\n",
    "    files = [f for f in os.listdir(corpus_dir) if f.endswith(\".txt\")]\n",
    "    matching_files = set()\n",
    "\n",
    "    for fname in tqdm(files, desc=f\"Searching for depth ≥ {target_depth}\"):\n",
    "        path = os.path.join(corpus_dir, fname)\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        sentence = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"<s\"):\n",
    "                sentence = []\n",
    "            elif line.startswith(\"</s>\"):\n",
    "                tree, roots = build_dependency_tree(sentence)\n",
    "                for root in roots:\n",
    "                    depth = get_max_depth(tree, root)\n",
    "                    if depth >= target_depth:\n",
    "                        matching_files.add(fname)\n",
    "                        break  # only print once per file\n",
    "            elif line:\n",
    "                sentence.append(line)\n",
    "\n",
    "    print(f\"\\n✅ Files with syntactic depth ≥ {target_depth}:\")\n",
    "    for f in sorted(matching_files):\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "print_files_with_depth_threshold(\n",
    "    corpus_dir=\"/home/volt/bach/pilot_data/COHA/10_20_parsed_1_SPOS\",\n",
    "    target_depth=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Collocation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dep_colloc.dep_colloc\n",
    "importlib.reload(dep_colloc.dep_colloc)\n",
    "from dep_colloc.dep_colloc import generate_path_colloc_df, generate_syn_colloc_df\n",
    "\n",
    "# corpus_dir = \"/home/volt/bach/pilot_data/test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Depth Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 10/10 [00:00<00:00, 771.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 427)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# save to CSV (or you could use df.to_pickle)\n",
    "import re\n",
    "pattern = re.compile(r'([^\\t]+)\\t([^\\t]+)\\t([^\\t])[^\\t]*\\t([^\\t]+)\\t([^\\t]+)\\t([^\\t]+)')\n",
    "df = generate_path_colloc_df(corpus_dir=corpus_dir, max_depth=1, pattern=pattern)\n",
    "print(df.shape)\n",
    "df.to_csv('./test_path.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Depth filtering + Dep Rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate frequency for the lemma and lemma_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dep_colloc.freq\n",
    "importlib.reload(dep_colloc.freq)\n",
    "from dep_colloc.freq import gen_lemma_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frequencies to /home/volt/bach/Embeddings/type_embeddings/w2vf/dep_w2v/COHA1020/lemma_deprel_freq.txt\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = '/home/volt/bach/pilot_data/COHA/10_20_parsed_1_SPOS'\n",
    "output_folder = '/home/volt/bach/Embeddings/type_embeddings/w2vf/dep_w2v/COHA1020'\n",
    "\n",
    "# corpus_folder = '/home/volt/bach/pilot_data/test'\n",
    "# output_folder = '/home/volt/bach/pilot_data/COHA/lemma_emb/w2vf/dep_w2v'\n",
    "# modes: 'lemma_pos', 'lemma_pos_init', or 'lemma_deprel'\n",
    "result_file = gen_lemma_freq(corpus_folder, output_folder,\n",
    "                                file_ext='.txt', mode='lemma_deprel')\n",
    "print(f\"Saved frequencies to {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected line format: '{'. Skipping.\n",
      "Warning: Unexpected line format: '}'. Skipping.\n",
      "\n",
      "Total sum of all counts: 52478970\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of tokens\n",
    "def count_total_value(filepath):\n",
    "    \"\"\"\n",
    "    Reads a text file with \"lemma/POS count\" format and calculates the total sum of the counts.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the input text file.\n",
    "\n",
    "    Returns:\n",
    "        int: The total sum of all counts. Returns 0 if the file is empty or no valid counts are found.\n",
    "    \"\"\"\n",
    "    total_value = 0\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()  # Remove leading/trailing whitespace\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "\n",
    "                parts = line.rsplit(':', 1) # Split from right, only once\n",
    "                \n",
    "                if len(parts) == 2:\n",
    "                    try:\n",
    "                        count_str = parts[1].strip()\n",
    "                        # Clean up potential extra spaces or non-digit characters around the number\n",
    "                        count = int(''.join(filter(str.isdigit, count_str)))\n",
    "                        total_value += count\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Could not parse count from line: '{line}'. Skipping.\")\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected line format: '{line}'. Skipping.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{filepath}'\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    return total_value\n",
    "\n",
    "# --- How to use the function ---\n",
    "\n",
    "# 2. Call the function with your file path\n",
    "file_path = \"/home/volt/bach/SynFlow/COHA_10_20.nfreq\"\n",
    "total_sum = count_total_value(file_path)\n",
    "\n",
    "if total_sum > 0:\n",
    "    print(f\"\\nTotal sum of all counts: {total_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the colloc matrix of lemma and lemma_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 10/10 [00:00<00:00, 1229.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 512)\n",
      "           declare/root  be/ccomp  ./punct  propose/ccomp  that/mark\n",
      "lemma_pos                                                           \n",
      "he/p                1.0       1.0      3.0            1.0        4.0\n",
      "there/e             1.0       1.0      0.0            0.0        0.0\n",
      "be/v                1.0       0.0      7.0            2.0        6.0\n",
      "mistake/n           1.0       1.0      0.0            0.0        0.0\n",
      "toss/v              1.0       1.0      0.0            0.0        0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = generate_syn_colloc_df(corpus_dir=corpus_dir, max_depth=2, pattern=pattern)\n",
    "print(df.shape)\n",
    "print(df.iloc[:5, :5])   # peek at top‐left corner\n",
    "df.to_csv('./test_syn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate (vocab, context) pairs for w2vf\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dep_contexts(csv_path, output_path):\n",
    "    # Đọc CSV (giả định cột đầu là vocab, dòng đầu là context)\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf8') as out:\n",
    "        for vocab, row in df.iterrows():\n",
    "            for context, count in row.items():\n",
    "                # Convert count về số nguyên nếu cần\n",
    "                count = int(round(float(count)))\n",
    "                if count > 0:\n",
    "                    for _ in range(count):\n",
    "                        out.write(f\"{vocab} {context}\\n\")\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "generate_dep_contexts(\"/home/volt/bach/pilot_data/COHA/lemma_emb/dep_colloc/test_syn.csv\", \"./dep.contexts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dep_colloc.ppmi\n",
    "importlib.reload(dep_colloc.ppmi)\n",
    "from dep_colloc.ppmi import PPMI_colloc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1229)\n"
     ]
    }
   ],
   "source": [
    "ppmi_df = PPMI_colloc_df(\n",
    "    dep_colloc_path=\"/home/volt/bach/pilot_data/COHA/lemma_emb/dep_colloc/test_syn.csv\",\n",
    "    lemma_pos_freq_path=\"/home/volt/bach/pilot_data/COHA/lemma_emb/dep_colloc/lemma_pos_freq.txt\",\n",
    "    # lemma_pos_deprel_freq_path=\"/home/volt/bach/pilot_data/COHA/lemma_emb/dep_colloc/lemma_deprel_freq.txt\",\n",
    "    min_count=1,\n",
    "    mode='lemma_pos'\n",
    ")\n",
    "print(ppmi_df.shape)\n",
    "# ppmi_df.to_csv('./test_ppmi_deprel.csv')\n",
    "ppmi_df.to_csv('./test_ppmi_path.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DataFrame to .pac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Add these only if you're not installing nephosem via pip\n",
    "sys.path.append(\"/home/volt/bach/KUL/nephosem\")\n",
    "sys.path.append(\"/home/volt/bach/KUL/semasioFlow\")\n",
    "from nephosem import TypeTokenMatrix\n",
    "\n",
    "def save_df_to_pac(df_path, output_path):\n",
    "    if output_path.endswith(\"/\"):\n",
    "        output_path = os.path.join(output_path, \"colloc_matrix\")\n",
    "    if not output_path.endswith(\".pac\"):\n",
    "        output_path += \".pac\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Load the DataFrame, using the first column as the index (row labels)\n",
    "    df = pd.read_csv(df_path, index_col=0)\n",
    "\n",
    "    # Build a COO and immediately convert to CSR\n",
    "    sparse_coo = coo_matrix(df.values)\n",
    "    sparse_csr = sparse_coo.tocsr()\n",
    "\n",
    "    row_items = df.index.tolist()\n",
    "    col_items = df.columns.tolist()\n",
    "\n",
    "    pac = TypeTokenMatrix(\n",
    "        matrix=sparse_csr,\n",
    "        row_items=row_items,\n",
    "        col_items=col_items\n",
    "    )\n",
    "    pac.save(output_path)\n",
    "    print(f\"[DONE] Saved .pac to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_pac(df_path='./test_syn.csv', output_path='./')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bython_ampere",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
